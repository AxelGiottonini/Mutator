{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllrrrrrrrr}\n",
      "\\toprule\n",
      " & mode & lr & bs & p & n_iter & d_mean & d_q1 & d_median & d_q3 & pearson & p_mut \\\\\n",
      "\\midrule\n",
      "75 & final & 0.0004 & 256 & 0.05 & 0 & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 \\\\\n",
      "68 & final & 0.0004 & 256 & 0.05 & 3 & 0.04 & 0.01 & 0.02 & 0.06 & 0.98 & 0.04 \\\\\n",
      "6 & final & 0.0004 & 256 & 0.05 & 5 & 0.08 & 0.02 & 0.05 & 0.10 & 0.93 & 0.06 \\\\\n",
      "16 & final & 0.0004 & 256 & 0.05 & 10 & 0.12 & 0.04 & 0.10 & 0.19 & 0.88 & 0.12 \\\\\n",
      "32 & final & 0.0004 & 256 & 0.05 & 20 & 0.20 & 0.08 & 0.14 & 0.28 & 0.50 & 0.20 \\\\\n",
      "23 & final & 0.0004 & 256 & 0.05 & 40 & 0.07 & 0.12 & 0.23 & 0.35 & 0.23 & 0.30 \\\\\n",
      "8 & final & 0.0004 & 256 & 0.05 & 80 & 0.38 & 0.25 & 0.36 & 0.47 & 0.26 & 0.39 \\\\\n",
      "4 & final & 0.0004 & 512 & 0.05 & 0 & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 \\\\\n",
      "78 & final & 0.0004 & 512 & 0.05 & 3 & 0.03 & 0.01 & 0.02 & 0.06 & 0.92 & 0.04 \\\\\n",
      "37 & final & 0.0004 & 512 & 0.05 & 5 & 0.07 & 0.01 & 0.04 & 0.10 & 0.93 & 0.06 \\\\\n",
      "70 & final & 0.0004 & 512 & 0.05 & 10 & 0.12 & 0.04 & 0.08 & 0.19 & 0.87 & 0.11 \\\\\n",
      "1 & final & 0.0004 & 512 & 0.05 & 20 & 0.20 & 0.08 & 0.15 & 0.28 & 0.67 & 0.19 \\\\\n",
      "47 & final & 0.0004 & 512 & 0.05 & 40 & 0.31 & 0.13 & 0.25 & 0.42 & 0.30 & 0.29 \\\\\n",
      "36 & final & 0.0004 & 512 & 0.05 & 80 & 0.39 & 0.23 & 0.34 & 0.50 & 0.45 & 0.39 \\\\\n",
      "61 & validation & 0.0004 & 128 & 0.05 & 0 & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 \\\\\n",
      "51 & validation & 0.0004 & 128 & 0.05 & 3 & 0.04 & 0.01 & 0.02 & 0.06 & 0.96 & 0.04 \\\\\n",
      "17 & validation & 0.0004 & 128 & 0.05 & 5 & 0.06 & 0.02 & 0.04 & 0.10 & 0.90 & 0.06 \\\\\n",
      "45 & validation & 0.0004 & 128 & 0.05 & 10 & 0.13 & 0.04 & 0.08 & 0.19 & 0.75 & 0.12 \\\\\n",
      "28 & validation & 0.0004 & 128 & 0.05 & 20 & 0.18 & 0.07 & 0.13 & 0.22 & 0.76 & 0.20 \\\\\n",
      "73 & validation & 0.0004 & 128 & 0.05 & 40 & 0.28 & 0.15 & 0.23 & 0.34 & 0.52 & 0.30 \\\\\n",
      "55 & validation & 0.0004 & 128 & 0.05 & 80 & 0.28 & 0.19 & 0.32 & 0.48 & 0.19 & 0.40 \\\\\n",
      "31 & validation & 0.001 & 256 & 0.05 & 0 & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 \\\\\n",
      "3 & validation & 0.001 & 256 & 0.05 & 3 & 0.06 & 0.01 & 0.03 & 0.07 & 0.94 & 0.04 \\\\\n",
      "58 & validation & 0.001 & 256 & 0.05 & 5 & 0.08 & 0.01 & 0.05 & 0.11 & 0.94 & 0.06 \\\\\n",
      "11 & validation & 0.001 & 256 & 0.05 & 10 & 0.14 & 0.05 & 0.10 & 0.20 & 0.90 & 0.11 \\\\\n",
      "22 & validation & 0.001 & 256 & 0.05 & 20 & 0.23 & 0.09 & 0.18 & 0.33 & 0.57 & 0.19 \\\\\n",
      "30 & validation & 0.001 & 256 & 0.05 & 40 & 0.35 & 0.19 & 0.31 & 0.46 & 0.50 & 0.30 \\\\\n",
      "40 & validation & 0.001 & 256 & 0.05 & 80 & 0.33 & 0.24 & 0.41 & 0.60 & 0.20 & 0.40 \\\\\n",
      "12 & final & 0.001 & 256 & 0.10 & 0 & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 \\\\\n",
      "7 & final & 0.001 & 256 & 0.10 & 3 & 0.07 & 0.01 & 0.03 & 0.09 & 0.97 & 0.04 \\\\\n",
      "50 & final & 0.001 & 256 & 0.10 & 5 & 0.09 & 0.02 & 0.05 & 0.13 & 0.94 & 0.06 \\\\\n",
      "67 & final & 0.001 & 256 & 0.10 & 10 & 0.18 & 0.06 & 0.12 & 0.25 & 0.80 & 0.11 \\\\\n",
      "52 & final & 0.001 & 256 & 0.10 & 20 & 0.27 & 0.11 & 0.21 & 0.34 & 0.80 & 0.19 \\\\\n",
      "71 & final & 0.001 & 256 & 0.10 & 40 & 0.39 & 0.19 & 0.34 & 0.53 & 0.61 & 0.30 \\\\\n",
      "82 & final & 0.001 & 256 & 0.10 & 80 & 0.49 & 0.27 & 0.44 & 0.66 & 0.50 & 0.40 \\\\\n",
      "14 & final & 0.001 & 512 & 0.10 & 0 & 0.00 & 0.00 & 0.00 & 0.00 & 0.98 & 0.00 \\\\\n",
      "83 & final & 0.001 & 512 & 0.10 & 3 & 0.07 & 0.02 & 0.05 & 0.11 & 0.97 & 0.04 \\\\\n",
      "56 & final & 0.001 & 512 & 0.10 & 5 & 0.11 & 0.02 & 0.06 & 0.17 & 0.93 & 0.06 \\\\\n",
      "76 & final & 0.001 & 512 & 0.10 & 10 & 0.18 & 0.04 & 0.13 & 0.28 & 0.84 & 0.12 \\\\\n",
      "20 & final & 0.001 & 512 & 0.10 & 20 & 0.29 & 0.10 & 0.23 & 0.39 & 0.67 & 0.19 \\\\\n",
      "81 & final & 0.001 & 512 & 0.10 & 40 & 0.46 & 0.22 & 0.40 & 0.57 & 0.28 & 0.29 \\\\\n",
      "13 & final & 0.001 & 512 & 0.10 & 80 & 0.56 & 0.33 & 0.50 & 0.73 & 0.22 & 0.41 \\\\\n",
      "43 & validation & 0.001 & 128 & 0.10 & 0 & 0.41 & 0.00 & 0.25 & 0.62 & 0.94 & 0.02 \\\\\n",
      "59 & validation & 0.001 & 128 & 0.10 & 3 & 1.08 & 0.62 & 0.88 & 1.38 & 0.79 & 0.06 \\\\\n",
      "41 & validation & 0.001 & 128 & 0.10 & 5 & 1.67 & 0.88 & 1.44 & 2.25 & 0.60 & 0.10 \\\\\n",
      "46 & validation & 0.001 & 128 & 0.10 & 10 & 2.95 & 1.72 & 2.69 & 3.94 & 0.23 & 0.17 \\\\\n",
      "66 & validation & 0.001 & 128 & 0.10 & 20 & 4.88 & 3.00 & 4.28 & 6.22 & -0.14 & 0.24 \\\\\n",
      "39 & validation & 0.001 & 128 & 0.10 & 40 & 6.75 & 5.02 & 6.44 & 8.16 & -0.25 & 0.33 \\\\\n",
      "27 & validation & 0.001 & 128 & 0.10 & 80 & 8.61 & 7.05 & 8.66 & 9.81 & -0.27 & 0.43 \\\\\n",
      "24 & validation & 0.001 & 256 & 0.10 & 0 & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 \\\\\n",
      "48 & validation & 0.001 & 256 & 0.10 & 3 & 0.06 & 0.02 & 0.04 & 0.10 & 0.95 & 0.04 \\\\\n",
      "62 & validation & 0.001 & 256 & 0.10 & 5 & 0.10 & 0.02 & 0.05 & 0.13 & 0.89 & 0.07 \\\\\n",
      "33 & validation & 0.001 & 256 & 0.10 & 10 & 0.16 & 0.05 & 0.10 & 0.23 & 0.93 & 0.12 \\\\\n",
      "74 & validation & 0.001 & 256 & 0.10 & 20 & 0.27 & 0.09 & 0.20 & 0.36 & 0.69 & 0.20 \\\\\n",
      "21 & validation & 0.001 & 256 & 0.10 & 40 & 0.38 & 0.20 & 0.33 & 0.54 & 0.58 & 0.30 \\\\\n",
      "10 & validation & 0.001 & 256 & 0.10 & 80 & 0.20 & 0.26 & 0.41 & 0.62 & 0.43 & 0.39 \\\\\n",
      "65 & final & 0.001 & 256 & 0.15 & 0 & 0.03 & 0.00 & 0.00 & 0.01 & 1.00 & 0.00 \\\\\n",
      "18 & final & 0.001 & 256 & 0.15 & 3 & 0.12 & 0.00 & 0.06 & 0.16 & 0.93 & 0.04 \\\\\n",
      "2 & final & 0.001 & 256 & 0.15 & 5 & 0.21 & 0.05 & 0.14 & 0.34 & 0.96 & 0.06 \\\\\n",
      "42 & final & 0.001 & 256 & 0.15 & 10 & 0.40 & 0.12 & 0.27 & 0.61 & 0.89 & 0.12 \\\\\n",
      "57 & final & 0.001 & 256 & 0.15 & 20 & 0.63 & 0.21 & 0.45 & 0.89 & 0.75 & 0.21 \\\\\n",
      "53 & final & 0.001 & 256 & 0.15 & 40 & 0.85 & 0.33 & 0.69 & 1.16 & 0.60 & 0.30 \\\\\n",
      "38 & final & 0.001 & 256 & 0.15 & 80 & 1.04 & 0.52 & 0.93 & 1.38 & 0.42 & 0.41 \\\\\n",
      "25 & final & 0.001 & 512 & 0.15 & 0 & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 \\\\\n",
      "15 & final & 0.001 & 512 & 0.15 & 3 & 0.09 & 0.02 & 0.06 & 0.11 & 0.98 & 0.04 \\\\\n",
      "80 & final & 0.001 & 512 & 0.15 & 5 & 0.15 & 0.03 & 0.08 & 0.20 & 0.94 & 0.07 \\\\\n",
      "0 & final & 0.001 & 512 & 0.15 & 10 & 0.23 & 0.07 & 0.14 & 0.34 & 0.89 & 0.12 \\\\\n",
      "54 & final & 0.001 & 512 & 0.15 & 20 & 0.38 & 0.16 & 0.28 & 0.49 & 0.83 & 0.20 \\\\\n",
      "63 & final & 0.001 & 512 & 0.15 & 40 & 0.54 & 0.23 & 0.43 & 0.72 & 0.52 & 0.30 \\\\\n",
      "9 & final & 0.001 & 512 & 0.15 & 80 & -1.25 & 0.30 & 0.52 & 0.85 & 0.28 & 0.41 \\\\\n",
      "79 & validation & 0.001 & 256 & 0.15 & 0 & 0.03 & 0.00 & 0.00 & 0.01 & 1.00 & 0.00 \\\\\n",
      "69 & validation & 0.001 & 256 & 0.15 & 3 & 0.12 & 0.02 & 0.08 & 0.17 & 0.95 & 0.04 \\\\\n",
      "64 & validation & 0.001 & 256 & 0.15 & 5 & 0.24 & 0.04 & 0.13 & 0.34 & 0.91 & 0.07 \\\\\n",
      "72 & validation & 0.001 & 256 & 0.15 & 10 & 0.41 & 0.10 & 0.29 & 0.60 & 0.86 & 0.12 \\\\\n",
      "77 & validation & 0.001 & 256 & 0.15 & 20 & 0.63 & 0.17 & 0.44 & 0.78 & 0.66 & 0.20 \\\\\n",
      "5 & validation & 0.001 & 256 & 0.15 & 40 & 0.87 & 0.38 & 0.62 & 1.14 & 0.57 & 0.30 \\\\\n",
      "35 & validation & 0.001 & 256 & 0.15 & 80 & 1.05 & 0.50 & 0.97 & 1.40 & 0.27 & 0.41 \\\\\n",
      "44 & validation & 0.001 & 512 & 0.15 & 0 & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 \\\\\n",
      "60 & validation & 0.001 & 512 & 0.15 & 3 & 0.09 & 0.01 & 0.05 & 0.12 & 0.97 & 0.04 \\\\\n",
      "34 & validation & 0.001 & 512 & 0.15 & 5 & 0.13 & 0.03 & 0.06 & 0.18 & 0.94 & 0.07 \\\\\n",
      "29 & validation & 0.001 & 512 & 0.15 & 10 & 0.25 & 0.06 & 0.15 & 0.35 & 0.88 & 0.12 \\\\\n",
      "49 & validation & 0.001 & 512 & 0.15 & 20 & 0.36 & 0.12 & 0.27 & 0.52 & 0.82 & 0.20 \\\\\n",
      "19 & validation & 0.001 & 512 & 0.15 & 40 & 0.51 & 0.20 & 0.38 & 0.67 & 0.56 & 0.30 \\\\\n",
      "26 & validation & 0.001 & 512 & 0.15 & 80 & 0.65 & 0.32 & 0.52 & 0.88 & 0.39 & 0.41 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns=[\"mode\", \"lr\", \"bs\", \"p\", \"n_iter\", \"d_mean\", \"d_q1\", \"d_median\", \"d_q3\", \"pearson\", \"p_mut\"]\n",
    "stats = []\n",
    "\n",
    "for f in glob.glob(\"./out/*.csv\"):\n",
    "    random_pattern = r\"\\.\\/out\\/inference.random.(?P<n_iter>\\d+).(?P<mode>(final)|(validation)).LR(?P<lr>.*)_BS(?P<bs>.*)_P(?P<p>.*).csv\"\n",
    "    base_pattern = r\"\\.\\/out\\/inference.(?P<mode>(final)|(validation)).LR(?P<lr>.*)_BS(?P<bs>.*)_P(?P<p>.*).csv\"\n",
    "    if attributes := re.match(random_pattern, f):\n",
    "        pass\n",
    "    elif attributes := re.match(base_pattern, f):\n",
    "        attributes = attributes.groupdict()\n",
    "        attributes[\"n_iter\"] = 0\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    df = pd.read_csv(f)\n",
    "    pearson =  sp.stats.pearsonr(df[\"perplexity_src\"], df[\"perplexity_mut\"]).statistic\n",
    "    dq1, dq3 = (df[\"perplexity_src\"] - df[\"perplexity_mut\"]).quantile(0.25), (df[\"perplexity_src\"] - df[\"perplexity_mut\"]).quantile(0.75)\n",
    "    dmean, dmedian = (df[\"perplexity_src\"] - df[\"perplexity_mut\"]).mean(), (df[\"perplexity_src\"] - df[\"perplexity_mut\"]).median()\n",
    "    \n",
    "    base_seq = pd.read_csv(\"./data/_nt_test.csv\").loc[:, \"seq\"].to_list()\n",
    "    mutated_seq = pd.read_csv(f)[\"sequences_mut\"].to_list()\n",
    "    p_mut = pd.Series([sum([base[i] != mut[i] for i in range(len(base))]) / len(base) for base, mut in zip(base_seq, mutated_seq) if len(base) == len(mut)]).mean()\n",
    "\n",
    "\n",
    "    entry = dict(zip(columns, [attributes['mode'], attributes['lr'], attributes['bs'], attributes['p'], attributes['n_iter'], dmean, dq1, dmedian, dq3, pearson, p_mut]))\n",
    "    stats.append(entry)\n",
    "\n",
    "\n",
    "\n",
    "stats = {key: [i[key] for i in stats] for key in stats[0]}\n",
    "stats = pd.DataFrame(stats)\n",
    "stats = stats.astype({'p': 'float32', 'n_iter': 'int32'})\n",
    "stats = stats.sort_values(by=[\"p\", \"mode\", \"lr\", \"bs\", \"n_iter\"])\n",
    "print(stats.to_latex(float_format=\"{:.2f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['S', 'D', 'R', 'T', 'G'], ['L', 'A', 'R', 'E', 'G'])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_seq = pd.read_csv(\"./data/_nt_test.csv\").loc[:, \"seq\"].to_list()\n",
    "mutated_seq = pd.read_csv(f)[\"sequences_mut\"].to_list()\n",
    "\n",
    "edges = [(base[i], mut[i]) for base, mut in zip(base_seq, mutated_seq) if len(base) == len(mut) for i in range(len(base)) if base[i] != mut[i]]\n",
    "_from, _to = set([el[0] for el in edges]), set([el[1] for el in edges])\n",
    "nodes = list(_from.union(_to))\n",
    "adjacency_matrix = {node: {n: 0 for n in nodes} for node in nodes}\n",
    "for base, mut in edges: adjacency_matrix[base][mut] += 1\n",
    "adjacency_matrix = [list(values.values()) for _, values in adjacency_matrix.items()]\n",
    "adjacency_matrix = np.array(adjacency_matrix)\n",
    "top_base= [nodes[i] for i in np.argsort(adjacency_matrix.sum(axis=1))[-5:]][::-1]\n",
    "top_mut = [nodes[i] for i in np.argsort(adjacency_matrix.sum(axis=0))[-5:]][::-1]\n",
    "top_base, top_mut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mut = list(zip(list(adjacency_matrix.flatten().argsort()[-20:] // len(adjacency_matrix)), list(adjacency_matrix.flatten().argsort()[-20:] % len(adjacency_matrix))))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('R', 'L', 12),\n",
       " ('G', 'L', 11),\n",
       " ('V', 'L', 10),\n",
       " ('D', 'L', 9),\n",
       " ('Q', 'L', 9),\n",
       " ('T', 'L', 9),\n",
       " ('F', 'L', 8),\n",
       " ('A', 'L', 8),\n",
       " ('V', 'A', 7),\n",
       " ('D', 'A', 7),\n",
       " ('P', 'L', 6),\n",
       " ('L', 'A', 6),\n",
       " ('S', 'E', 6),\n",
       " ('S', 'L', 6),\n",
       " ('E', 'L', 5),\n",
       " ('S', 'A', 5),\n",
       " ('E', 'G', 5),\n",
       " ('N', 'K', 5),\n",
       " ('T', 'E', 5),\n",
       " ('S', 'R', 5)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(nodes[base], nodes[mut], adjacency_matrix[base, mut]) for base, mut in top_mut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency_matrix[14, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllrrrrrrr}\n",
      "\\toprule\n",
      "mode & lr & bs & p & n_iter & d_mean & d_q1 & d_median & d_q3 & pearson \\\\\n",
      "\\midrule\n",
      "final & 0.0004 & 256 & 0.050 & 0 & 0.001 & 0.000 & 0.000 & 0.000 & 1.000 \\\\\n",
      "final & 0.0004 & 256 & 0.050 & 3 & 0.039 & 0.008 & 0.023 & 0.062 & 0.979 \\\\\n",
      "final & 0.0004 & 256 & 0.050 & 5 & 0.078 & 0.016 & 0.055 & 0.102 & 0.935 \\\\\n",
      "final & 0.0004 & 256 & 0.050 & 10 & 0.125 & 0.039 & 0.102 & 0.188 & 0.876 \\\\\n",
      "final & 0.0004 & 512 & 0.050 & 0 & 0.000 & 0.000 & 0.000 & 0.000 & 1.000 \\\\\n",
      "final & 0.0004 & 512 & 0.050 & 3 & 0.030 & 0.006 & 0.023 & 0.057 & 0.916 \\\\\n",
      "final & 0.0004 & 512 & 0.050 & 5 & 0.066 & 0.008 & 0.039 & 0.102 & 0.934 \\\\\n",
      "final & 0.0004 & 512 & 0.050 & 10 & 0.123 & 0.039 & 0.082 & 0.188 & 0.873 \\\\\n",
      "validation & 0.0004 & 128 & 0.050 & 0 & 0.003 & 0.000 & 0.000 & 0.000 & 0.996 \\\\\n",
      "validation & 0.0004 & 128 & 0.050 & 3 & 0.039 & 0.008 & 0.023 & 0.062 & 0.955 \\\\\n",
      "validation & 0.0004 & 128 & 0.050 & 5 & 0.064 & 0.016 & 0.043 & 0.102 & 0.895 \\\\\n",
      "validation & 0.0004 & 128 & 0.050 & 10 & 0.132 & 0.037 & 0.078 & 0.191 & 0.755 \\\\\n",
      "validation & 0.001 & 256 & 0.050 & 0 & 0.001 & 0.000 & 0.000 & 0.000 & 0.999 \\\\\n",
      "validation & 0.001 & 256 & 0.050 & 3 & 0.057 & 0.008 & 0.031 & 0.072 & 0.938 \\\\\n",
      "validation & 0.001 & 256 & 0.050 & 5 & 0.076 & 0.008 & 0.047 & 0.109 & 0.943 \\\\\n",
      "validation & 0.001 & 256 & 0.050 & 10 & 0.140 & 0.053 & 0.098 & 0.203 & 0.897 \\\\\n",
      "final & 0.001 & 256 & 0.100 & 0 & 0.001 & 0.000 & 0.000 & 0.000 & 1.000 \\\\\n",
      "final & 0.001 & 256 & 0.100 & 3 & 0.066 & 0.008 & 0.031 & 0.088 & 0.971 \\\\\n",
      "final & 0.001 & 256 & 0.100 & 5 & 0.092 & 0.016 & 0.051 & 0.133 & 0.937 \\\\\n",
      "final & 0.001 & 256 & 0.100 & 10 & 0.185 & 0.062 & 0.125 & 0.252 & 0.798 \\\\\n",
      "final & 0.001 & 512 & 0.100 & 0 & 0.001 & 0.000 & 0.000 & 0.000 & 0.985 \\\\\n",
      "final & 0.001 & 512 & 0.100 & 3 & 0.066 & 0.016 & 0.047 & 0.109 & 0.971 \\\\\n",
      "final & 0.001 & 512 & 0.100 & 5 & 0.106 & 0.023 & 0.062 & 0.166 & 0.926 \\\\\n",
      "final & 0.001 & 512 & 0.100 & 10 & 0.176 & 0.045 & 0.129 & 0.275 & 0.843 \\\\\n",
      "validation & 0.001 & 128 & 0.100 & 0 & 0.414 & 0.000 & 0.250 & 0.625 & 0.937 \\\\\n",
      "validation & 0.001 & 128 & 0.100 & 3 & 1.084 & 0.625 & 0.875 & 1.375 & 0.788 \\\\\n",
      "validation & 0.001 & 128 & 0.100 & 5 & 1.673 & 0.875 & 1.438 & 2.250 & 0.598 \\\\\n",
      "validation & 0.001 & 128 & 0.100 & 10 & 2.949 & 1.719 & 2.688 & 3.938 & 0.229 \\\\\n",
      "validation & 0.001 & 256 & 0.100 & 0 & 0.001 & 0.000 & 0.000 & 0.000 & 1.000 \\\\\n",
      "validation & 0.001 & 256 & 0.100 & 3 & 0.060 & 0.016 & 0.039 & 0.102 & 0.954 \\\\\n",
      "validation & 0.001 & 256 & 0.100 & 5 & 0.097 & 0.023 & 0.047 & 0.127 & 0.890 \\\\\n",
      "validation & 0.001 & 256 & 0.100 & 10 & 0.161 & 0.055 & 0.102 & 0.234 & 0.927 \\\\\n",
      "final & 0.001 & 256 & 0.150 & 0 & 0.033 & 0.000 & 0.000 & 0.010 & 0.996 \\\\\n",
      "final & 0.001 & 256 & 0.150 & 3 & 0.120 & 0.000 & 0.062 & 0.156 & 0.934 \\\\\n",
      "final & 0.001 & 256 & 0.150 & 5 & 0.207 & 0.047 & 0.141 & 0.336 & 0.958 \\\\\n",
      "final & 0.001 & 256 & 0.150 & 10 & 0.398 & 0.115 & 0.270 & 0.609 & 0.894 \\\\\n",
      "final & 0.001 & 512 & 0.150 & 0 & 0.001 & 0.000 & 0.000 & 0.000 & 0.999 \\\\\n",
      "final & 0.001 & 512 & 0.150 & 3 & 0.091 & 0.016 & 0.059 & 0.113 & 0.981 \\\\\n",
      "final & 0.001 & 512 & 0.150 & 5 & 0.150 & 0.031 & 0.078 & 0.197 & 0.945 \\\\\n",
      "final & 0.001 & 512 & 0.150 & 10 & 0.227 & 0.068 & 0.145 & 0.338 & 0.889 \\\\\n",
      "validation & 0.001 & 256 & 0.150 & 0 & 0.033 & 0.000 & 0.000 & 0.010 & 0.996 \\\\\n",
      "validation & 0.001 & 256 & 0.150 & 3 & 0.121 & 0.016 & 0.078 & 0.172 & 0.951 \\\\\n",
      "validation & 0.001 & 256 & 0.150 & 5 & 0.235 & 0.043 & 0.133 & 0.344 & 0.906 \\\\\n",
      "validation & 0.001 & 256 & 0.150 & 10 & 0.407 & 0.100 & 0.293 & 0.598 & 0.863 \\\\\n",
      "validation & 0.001 & 512 & 0.150 & 0 & 0.001 & 0.000 & 0.000 & 0.000 & 0.999 \\\\\n",
      "validation & 0.001 & 512 & 0.150 & 3 & 0.085 & 0.014 & 0.047 & 0.125 & 0.971 \\\\\n",
      "validation & 0.001 & 512 & 0.150 & 5 & 0.132 & 0.031 & 0.062 & 0.176 & 0.937 \\\\\n",
      "validation & 0.001 & 512 & 0.150 & 10 & 0.251 & 0.062 & 0.148 & 0.348 & 0.875 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(stats.to_latex(index=False, float_format=\"{:.3f}\".format))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mes2therm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
